/*
 * This file was generated by the Gradle 'init' task.
 *
 * This generated file contains a sample Java application project to get you started.
 * For more details on building Java & JVM projects, please refer to https://docs.gradle.org/8.14.1/userguide/building_java_projects.html in the Gradle documentation.
 */


plugins {
    // Apply the application plugin to add support for building a CLI application in Java.
    application
}

repositories {
    // Use Maven Central for resolving dependencies.
    mavenCentral()
    
    // Add Confluent repository for Kafka dependencies
    maven {
        url = uri("https://packages.confluent.io/maven/")
    }
    
    // Add repository for Pentaho dependencies (required by Hive)
    maven {
        url = uri("https://repository.cloudera.com/artifactory/cloudera-repos/")
    }
    
    // Add Apache Snapshots repository
    maven {
        url = uri("https://repository.apache.org/content/repositories/snapshots/")
    }
    
    // Add JCenter as fallback for older dependencies
    gradlePluginPortal()
}

dependencies {
    // Use JUnit Jupiter for testing.
    testImplementation(libs.junit.jupiter)
    testRuntimeOnly("org.junit.platform:junit-platform-launcher")
    
    // Hadoop testing utilities
    testImplementation(libs.hadoop.minicluster)
    testImplementation("org.apache.hadoop:hadoop-hdfs:3.3.6:tests")
    testImplementation("org.apache.hadoop:hadoop-common:3.3.6:tests")

    // Basic utilities
    implementation(libs.guava)
    
    // Hadoop ecosystem
    implementation(libs.hadoop.client)
    implementation(libs.hadoop.hdfs)
    implementation(libs.hadoop.common)
    
    // Spark (provided by cluster at runtime)
    compileOnly(libs.spark.core)
    compileOnly(libs.spark.sql)
    compileOnly(libs.spark.streaming)
    compileOnly(libs.spark.streaming.kafka)
    
    // For testing, we need implementation scope
    testImplementation(libs.spark.core)
    testImplementation(libs.spark.sql)
    testImplementation(libs.spark.streaming)
    testImplementation(libs.spark.streaming.kafka)
    
    // Kafka
    implementation(libs.kafka.clients)
    implementation(libs.kafka.streams)
    
    // Hive dependencies (newer version for JDK 17 compatibility)
    implementation("org.apache.hive:hive-jdbc:3.1.3")
    implementation("org.apache.hive:hive-metastore:3.1.3") 
    implementation("org.apache.hive:hive-exec:3.1.3")
    
    // Apache Calcite
    implementation("org.apache.calcite:calcite-core:1.35.0")
    implementation("org.apache.calcite:calcite-druid:1.35.0")
    implementation("org.apache.calcite:calcite-file:1.35.0")
    implementation("org.apache.calcite:calcite-linq4j:1.35.0")
    
    // JSON processing
    implementation(libs.jackson.core)
    implementation(libs.jackson.databind)
    implementation(libs.jackson.annotations)
    implementation(libs.jackson.jsr310)
    
    // Logging
    implementation(libs.slf4j.api)
    implementation(libs.slf4j.simple)
    
    // Code generation
    compileOnly(libs.lombok)
    annotationProcessor(libs.lombok)
    testCompileOnly(libs.lombok)
    testAnnotationProcessor(libs.lombok)
    
    // Null safety annotations
    implementation(libs.jsr305)
    
    // Validation
    implementation(libs.validation.api)
    implementation(libs.hibernate.validator)
    implementation(libs.jakarta.el)
}

// Apply a specific Java toolchain to ease working on different environments.
java {
    toolchain {
        languageVersion = JavaLanguageVersion.of(17)
    }
}

application {
    // Define the main class for the application.
    mainClass = "org.example.App"
}

tasks.named<Test>("test") {
    // Use JUnit Platform for unit tests.
    useJUnitPlatform {
        // Exclude Docker tests from regular test task
        excludeTags("docker-test")
    }
    
    // Add JVM arguments for Java 17 compatibility with Spark 3.3.0
    jvmArgs(
        "--add-opens=java.base/java.lang=ALL-UNNAMED",
        "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED", 
        "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED",
        "--add-opens=java.base/java.nio=ALL-UNNAMED",
        "--add-opens=java.base/java.util=ALL-UNNAMED",
        "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED"
    )
}

// Manual Docker service control tasks (optional - extension handles auto-setup)
tasks.register<Exec>("startServices") {
    group = "docker"
    description = "Manually start all Docker services (Hadoop + Spark)"
    
    workingDir = file("../")
    commandLine("docker", "compose", "up", "-d")
    
    doLast {
        println("ðŸš€ All Docker services started manually!")
        println("   - Namenode UI: http://localhost:9870")
        println("   - Spark Master UI: http://localhost:8080")
        println("   - HDFS endpoint: hdfs://localhost:9000")
        println("   - Spark cluster: spark://localhost:7077")
        println("   - Wait a moment for services to be ready")
    }
}

tasks.register<Exec>("stopServices") {
    group = "docker"
    description = "Manually stop all Docker services"
    
    workingDir = file("../")
    commandLine("docker", "compose", "down")
    
    doLast {
        println("ðŸ›‘ All Docker services stopped manually")
    }
}

tasks.register<Test>("testDocker") {
    group = "docker"
    description = "Run Docker integration tests (tests tagged with 'docker-test')"
    
    useJUnitPlatform {
        includeTags("docker-test")
    }
    
    // Add JVM arguments for Java 17 compatibility with Spark 3.3.0
    jvmArgs(
        "--add-opens=java.base/java.lang=ALL-UNNAMED",
        "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED", 
        "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED",
        "--add-opens=java.base/java.nio=ALL-UNNAMED",
        "--add-opens=java.base/java.util=ALL-UNNAMED",
        "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED"
    )
    
    doFirst {
        println("ðŸ§ª Running tests tagged with 'docker-test'...")
        println("   Docker services will be started automatically by test extensions")
    }
}

// Alias for testDocker - extension handles everything
tasks.register("dockerTestFull") {
    group = "docker" 
    description = "Run Docker tests (alias for testDocker - extension handles setup)"
    dependsOn("testDocker")
}

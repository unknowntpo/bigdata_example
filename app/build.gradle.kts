/*
 * This file was generated by the Gradle 'init' task.
 *
 * This generated file contains a sample Java application project to get you started.
 * For more details on building Java & JVM projects, please refer to https://docs.gradle.org/8.14.1/userguide/building_java_projects.html in the Gradle documentation.
 */


plugins {
    // Apply the application plugin to add support for building a CLI application in Java.
    application
}

repositories {
    // Use Maven Central for resolving dependencies.
    mavenCentral()
    
    // Add Confluent repository for Kafka dependencies
    maven {
        url = uri("https://packages.confluent.io/maven/")
    }
    
    // Add repository for Pentaho dependencies (required by Hive)
    maven {
        url = uri("https://repository.cloudera.com/artifactory/cloudera-repos/")
    }
    
    // Add Apache Snapshots repository
    maven {
        url = uri("https://repository.apache.org/content/repositories/snapshots/")
    }
    
    // Add JCenter as fallback for older dependencies
    gradlePluginPortal()
}

dependencies {
    // Use JUnit Jupiter for testing.
    testImplementation(libs.junit.jupiter)
    testRuntimeOnly("org.junit.platform:junit-platform-launcher")
    
    // Hadoop testing utilities
    testImplementation(libs.hadoop.minicluster)
    testImplementation("org.apache.hadoop:hadoop-hdfs:3.3.6:tests")
    testImplementation("org.apache.hadoop:hadoop-common:3.3.6:tests")

    // Basic utilities
    implementation(libs.guava)
    
    // Hadoop ecosystem
    implementation(libs.hadoop.client)
    implementation(libs.hadoop.hdfs)
    implementation(libs.hadoop.common)
    
    // Spark
    implementation(libs.spark.core)
    implementation(libs.spark.sql)
    implementation(libs.spark.streaming)
    implementation(libs.spark.streaming.kafka)
    
    // Kafka
    implementation(libs.kafka.clients)
    implementation(libs.kafka.streams)
    
    // Hive with exclusions for problematic dependencies
    implementation("org.apache.hive:hive-jdbc:2.3.9") {
        exclude(group = "org.pentaho", module = "pentaho-aggdesigner-algorithm")
    }
    implementation("org.apache.hive:hive-metastore:2.3.9") {
        exclude(group = "org.pentaho", module = "pentaho-aggdesigner-algorithm")
    }
    implementation("org.apache.hive:hive-exec:2.3.9") {
        exclude(group = "org.pentaho", module = "pentaho-aggdesigner-algorithm")
        exclude(group = "org.apache.calcite", module = "calcite-core")
        exclude(group = "org.apache.calcite", module = "calcite-druid")
    }
    
    // Apache Calcite
    implementation("org.apache.calcite:calcite-core:1.35.0")
    implementation("org.apache.calcite:calcite-druid:1.35.0")
    implementation("org.apache.calcite:calcite-file:1.35.0")
    implementation("org.apache.calcite:calcite-linq4j:1.35.0")
    
    // JSON processing
    implementation(libs.jackson.core)
    implementation(libs.jackson.databind)
    implementation(libs.jackson.annotations)
    implementation(libs.jackson.jsr310)
    
    // Logging
    implementation(libs.slf4j.api)
    implementation(libs.slf4j.simple)
    
    // Code generation
    compileOnly(libs.lombok)
    annotationProcessor(libs.lombok)
    testCompileOnly(libs.lombok)
    testAnnotationProcessor(libs.lombok)
    
    // Null safety annotations
    implementation(libs.jsr305)
    
    // Validation
    implementation(libs.validation.api)
    implementation(libs.hibernate.validator)
    implementation(libs.jakarta.el)
}

// Apply a specific Java toolchain to ease working on different environments.
java {
    toolchain {
        languageVersion = JavaLanguageVersion.of(21)
    }
}

application {
    // Define the main class for the application.
    mainClass = "org.example.App"
}

tasks.named<Test>("test") {
    // Use JUnit Platform for unit tests.
    useJUnitPlatform {
        // Exclude Hadoop tests from regular test task
        excludeTags("hadoop-docker-test")
    }
}

// Manual Hadoop control tasks (optional - extension handles auto-setup)
tasks.register<Exec>("startHadoop") {
    group = "hadoop"
    description = "Manually start Hadoop services (namenode and datanode)"
    
    workingDir = file("../")
    commandLine("docker", "compose", "up", "-d", "namenode", "datanode")
    
    doLast {
        println("ðŸš€ Hadoop services started manually!")
        println("   - Namenode UI: http://localhost:9870")
        println("   - HDFS endpoint: hdfs://localhost:9000")
        println("   - Wait a moment for services to be ready")
    }
}

tasks.register<Exec>("stopHadoop") {
    group = "hadoop"
    description = "Manually stop Hadoop services"
    
    workingDir = file("../")
    commandLine("docker", "compose", "down")
    
    doLast {
        println("ðŸ›‘ Hadoop services stopped manually")
    }
}

tasks.register<Test>("testHadoop") {
    group = "hadoop"
    description = "Run Hadoop integration tests (tests tagged with 'hadoop-docker-test')"
    
    useJUnitPlatform {
        includeTags("hadoop-docker-test")
    }
    
    doFirst {
        println("ðŸ§ª Running tests tagged with 'hadoop-docker-test'...")
        println("   Hadoop services will be started automatically by test extension")
    }
}

// Alias for testHadoop - extension handles everything
tasks.register("hadoopTestFull") {
    group = "hadoop" 
    description = "Run Hadoop tests (alias for testHadoop - extension handles setup)"
    dependsOn("testHadoop")
}
